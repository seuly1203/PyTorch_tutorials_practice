{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Automatic_Differentiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation with torch.autograd"
      ],
      "metadata": {
        "id": "WrgeJuwCOHwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인공신경망을 학습시킬 때 가장 흔하게 사용되는 알고리즘: back propagation\n",
        "- Back propagation에서 parameters(model weights)가 gradient of the loss function에 따라 조정됨(with respoect to the given parameter)\n",
        "\n",
        "- 이러한 gradient들을 계산하기 위해서 PyTorch에 빌트인 되어있는게 torch.autograd\n",
        "  - it supports automatic computation of gradient for any computational graph\n",
        "\n"
      ],
      "metadata": {
        "id": "o0VJyjZFQO2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors, Functions and Computational graph"
      ],
      "metadata": {
        "id": "7BHYeVKwONcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5) # input tensor\n",
        "y = torch.zeros(3) # expected output\n",
        "\n",
        "\n",
        "w = torch.randn(5, 3, requires_grad = True) # parameter1 \n",
        "b = torch.randn(3, requires_grad = True) # parameter2\n",
        "# w, b: paramters which need to be optimized\n",
        "# -> need to compute the gradients of loss function with respect to those(w,b)\n",
        "# -> to do this: set the [requires_grad] property of those tensors\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n"
      ],
      "metadata": {
        "id": "yxNbf1ICOZ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to construct computational graph -> apply an object of class Function\n",
        "# which knows how to compute the function in the forward direction, and also\n",
        "# how to compute its derivative during the backward propagation step\n",
        "\n",
        "# a reference to the pack propagation function is stored in [grad_fn]\n",
        "print(f'Gradient function for z = {z.grad_fn}')\n",
        "print(f'Gradient function for loss = {loss.grad_fn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgXbkPMkOagC",
        "outputId": "19f8e50d-2190-441c-8d21-9dbebc4d4865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7fbc4cc279d0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fbc4cc27c90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Gradients"
      ],
      "metadata": {
        "id": "K5RLnohfOQZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터들의 가중치를 최적화하기 위해서 -> 손실 함수의 derivatives를 계산해야함\n",
        "loss.backward() # to compute these derivatives\n",
        "print(w.grad) # to retrive the values \n",
        "print(b.grad) # to retrive the values "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZqpWqcOa-L",
        "outputId": "2a5aa941-6106-4dce-cb58-cccb9f045ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2836, 0.0166, 0.1387],\n",
            "        [0.2836, 0.0166, 0.1387],\n",
            "        [0.2836, 0.0166, 0.1387],\n",
            "        [0.2836, 0.0166, 0.1387],\n",
            "        [0.2836, 0.0166, 0.1387]])\n",
            "tensor([0.2836, 0.0166, 0.1387])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disabling Gradient Tracking\n",
        "\n",
        "- By default, all tensors with requires_grad = True are tracking their computational hisotry and support gradient computation.\n",
        "- However, there are some cases when we do not need to do that, for example, when we have trained the model and just wnat to apply it to some input data,i.e. we only want to do [forward] computations through the network. \n",
        "- We can stop tracking computations by surrounding out computation code with [torch.no_grad()] block"
      ],
      "metadata": {
        "id": "7dyIPbjuOSO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "# gradient 계산하지 않아도 될때: i.e. [forward] 사용할 때 -> no_grad()로 계산 코드를 감싸줌으로써 계산 트래킹 중단\n",
        "with torch.no_grad():\n",
        "  z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG_wbwqnObHS",
        "outputId": "e46032ea-23fd-47a3-9fdf-17c230f6b0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to achieve the same result: [detach()]\n",
        "z = torch.matmul(x, w) + b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upP2ZPTWEWA5",
        "outputId": "21432960-3b0b-46a9-c753-8184d7f43cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  gradient tracking을 중단하는 이유:\n",
        "  #1. 신경망의 일부 파라미터를 frozen시키기 위해서 - 사전학습 네트워크 파인튜닝에서 흔한 경우\n",
        "  #2. forward pass만 할 경우 계산 속도를 향상시키기 위해서 - gradient tracking하지 않는 게 훨씬 효율적"
      ],
      "metadata": {
        "id": "PDSzoe_NElCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on computational Graphs\n",
        "\n",
        "Conceptually, autograd keeps a record of data(tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. \n",
        "\n",
        "In this DAG, \n",
        "- leaves: input tensors\n",
        "- roots: output tensors. \n",
        "- By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."
      ],
      "metadata": {
        "id": "_VNfwU3fOUl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a forward pass, autograd does two things simultaneously:\n",
        "- run the requested operation to compute a resulting tensor\n",
        "- maintin the operation's gradient function in the DAG\n",
        "\n",
        "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
        "- computes the gradients from each .grad_fn\n",
        "- accumulates them in the respective tensor's .grad attribute\n",
        "- using the chain rule, propagates all they way to the leaf tensors"
      ],
      "metadata": {
        "id": "Wjd_rQKsJeDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed."
      ],
      "metadata": {
        "id": "7hlA3bwGJil9"
      }
    }
  ]
}