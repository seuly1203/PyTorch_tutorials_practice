{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Optimization_Loop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimiaing Model Parameters\n",
        "\n",
        "Training a model is an iterative process; \n",
        "in each iteration(called an epoch) the model\n",
        "- makes a guess about the output, \n",
        "- calculates the error in its guess(loss)\n",
        "- collects the derivatives of the error with respect to its parameters\n",
        "- optimizes the parameters using gradient descent"
      ],
      "metadata": {
        "id": "NvXU-3ZrKIY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite Code"
      ],
      "metadata": {
        "id": "VtcKvI0JK5d4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWVin-7JZsx"
      },
      "outputs": [],
      "source": [
        "# Datasets & DataLoaders, Build Model에서 사용한 코드\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameterse\n",
        "\n",
        "하이퍼파라미터는 변경 가능한 파라미터로 모델의 최적화를 위해 조정할 수 있다. 여러가지 하이퍼 파라미터 값들은 모델 학습과 convergence rates에 영향을 줄 수 있다.\n",
        "\n",
        "- rate of convergence: a measure of how the difference between the solution point and its estimates goes to zero"
      ],
      "metadata": {
        "id": "MmqqEwAPK7y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습을 위해 다음의 하이퍼 파라미터들을 튜닝한다:\n",
        "- Number of Epochs\n",
        "  - the number times to iterate over the dataset\n",
        "- Batch size\n",
        "  - the number of data samples propagated through the network before the parameters are updated\n",
        "- Learnng Rate\n",
        "  - how much to update models parameters at each batch/epoch. 값이 작을 수록 느린 학습"
      ],
      "metadata": {
        "id": "zqIUoxi3NS5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "acdaVyPhO2Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimiaztion Loop\n",
        "\n",
        "하이퍼 파라미터를 설정하면 optimization loop을 사용해 모델을 학습, 최적화시킬 수 있다.\n",
        "각각의 iteration == epoch\n",
        "\n",
        "하나의 epoch는 두 가지 메인 파트로 구성:\n",
        "- The Train Loop: iterate over the training and try to converge to optimal parametes\n",
        "- The Validation/Test Loop: iterate over the test dataset to check if model performance is improving"
      ],
      "metadata": {
        "id": "NcEX_V57K_XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "r5UtKib1Qu13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "\n",
        "Optimization\n",
        "- the process of adjusting model parameters to reduce modle error in each training step\n",
        "\n",
        "Optimization algorithms으로 어떻게 최적화를 할 것인지 정의\n",
        "\n",
        "All optimization logic is encapulated in the optimizer object"
      ],
      "metadata": {
        "id": "nOIskNX8Qwhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "# PyTorch에서 ADAM, RMSProp 같은 다른 optimizers 사용가능"
      ],
      "metadata": {
        "id": "Gp3qgbTpRrQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Implementation"
      ],
      "metadata": {
        "id": "VVDdG3kILBWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full implementation of train_loop \n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation - Optimization process\n",
        "        optimizer.zero_grad() # to rest the gradients of model parameters\n",
        "        loss.backward() # backpropagate the prediction loass with a call to loss.backward()\n",
        "        optimizer.step() # to adject the parameters by the gradient collected in the backward pass\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "SU8gxmFkPkB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the loss function and optimzer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# pass the loss function and optimzer to train_loop and test_loop\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dnrS_zYS4PY",
        "outputId": "4b5e0e22-7001-46b8-8449-10af81faf150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298863  [    0/60000]\n",
            "loss: 2.286261  [ 6400/60000]\n",
            "loss: 2.269766  [12800/60000]\n",
            "loss: 2.264961  [19200/60000]\n",
            "loss: 2.252468  [25600/60000]\n",
            "loss: 2.223565  [32000/60000]\n",
            "loss: 2.229445  [38400/60000]\n",
            "loss: 2.197865  [44800/60000]\n",
            "loss: 2.195070  [51200/60000]\n",
            "loss: 2.167722  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.4%, Avg loss: 2.160815 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.168206  [    0/60000]\n",
            "loss: 2.157894  [ 6400/60000]\n",
            "loss: 2.105493  [12800/60000]\n",
            "loss: 2.121247  [19200/60000]\n",
            "loss: 2.072092  [25600/60000]\n",
            "loss: 2.014868  [32000/60000]\n",
            "loss: 2.042094  [38400/60000]\n",
            "loss: 1.966685  [44800/60000]\n",
            "loss: 1.970884  [51200/60000]\n",
            "loss: 1.899888  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.3%, Avg loss: 1.894716 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.930654  [    0/60000]\n",
            "loss: 1.895056  [ 6400/60000]\n",
            "loss: 1.783226  [12800/60000]\n",
            "loss: 1.820130  [19200/60000]\n",
            "loss: 1.710875  [25600/60000]\n",
            "loss: 1.664680  [32000/60000]\n",
            "loss: 1.692376  [38400/60000]\n",
            "loss: 1.594608  [44800/60000]\n",
            "loss: 1.617955  [51200/60000]\n",
            "loss: 1.513327  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 1.524457 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.596684  [    0/60000]\n",
            "loss: 1.553428  [ 6400/60000]\n",
            "loss: 1.409527  [12800/60000]\n",
            "loss: 1.473559  [19200/60000]\n",
            "loss: 1.357026  [25600/60000]\n",
            "loss: 1.356100  [32000/60000]\n",
            "loss: 1.375270  [38400/60000]\n",
            "loss: 1.301397  [44800/60000]\n",
            "loss: 1.329854  [51200/60000]\n",
            "loss: 1.234668  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.254740 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.333618  [    0/60000]\n",
            "loss: 1.309597  [ 6400/60000]\n",
            "loss: 1.150621  [12800/60000]\n",
            "loss: 1.247393  [19200/60000]\n",
            "loss: 1.126922  [25600/60000]\n",
            "loss: 1.155054  [32000/60000]\n",
            "loss: 1.179939  [38400/60000]\n",
            "loss: 1.118408  [44800/60000]\n",
            "loss: 1.150322  [51200/60000]\n",
            "loss: 1.070934  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.087799 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.158483  [    0/60000]\n",
            "loss: 1.154843  [ 6400/60000]\n",
            "loss: 0.979134  [12800/60000]\n",
            "loss: 1.106681  [19200/60000]\n",
            "loss: 0.983170  [25600/60000]\n",
            "loss: 1.021278  [32000/60000]\n",
            "loss: 1.058882  [38400/60000]\n",
            "loss: 1.002443  [44800/60000]\n",
            "loss: 1.033854  [51200/60000]\n",
            "loss: 0.968086  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.980299 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.038140  [    0/60000]\n",
            "loss: 1.054807  [ 6400/60000]\n",
            "loss: 0.862530  [12800/60000]\n",
            "loss: 1.012974  [19200/60000]\n",
            "loss: 0.892381  [25600/60000]\n",
            "loss: 0.928380  [32000/60000]\n",
            "loss: 0.979950  [38400/60000]\n",
            "loss: 0.927828  [44800/60000]\n",
            "loss: 0.953607  [51200/60000]\n",
            "loss: 0.899781  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.907417 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.950646  [    0/60000]\n",
            "loss: 0.985683  [ 6400/60000]\n",
            "loss: 0.779943  [12800/60000]\n",
            "loss: 0.946423  [19200/60000]\n",
            "loss: 0.832788  [25600/60000]\n",
            "loss: 0.861134  [32000/60000]\n",
            "loss: 0.925028  [38400/60000]\n",
            "loss: 0.878393  [44800/60000]\n",
            "loss: 0.895741  [51200/60000]\n",
            "loss: 0.851128  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.6%, Avg loss: 0.855184 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.883946  [    0/60000]\n",
            "loss: 0.934215  [ 6400/60000]\n",
            "loss: 0.719067  [12800/60000]\n",
            "loss: 0.896896  [19200/60000]\n",
            "loss: 0.791089  [25600/60000]\n",
            "loss: 0.810662  [32000/60000]\n",
            "loss: 0.883538  [38400/60000]\n",
            "loss: 0.844078  [44800/60000]\n",
            "loss: 0.852454  [51200/60000]\n",
            "loss: 0.814254  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.815733 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.830831  [    0/60000]\n",
            "loss: 0.893320  [ 6400/60000]\n",
            "loss: 0.672107  [12800/60000]\n",
            "loss: 0.858659  [19200/60000]\n",
            "loss: 0.759917  [25600/60000]\n",
            "loss: 0.771524  [32000/60000]\n",
            "loss: 0.850121  [38400/60000]\n",
            "loss: 0.818718  [44800/60000]\n",
            "loss: 0.818782  [51200/60000]\n",
            "loss: 0.784932  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.784428 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}